{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772b7331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/graphrag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:nano-graphrag:Loading tokenizer: type='tiktoken', name='gpt-4o'\n",
      "INFO:nano-graphrag:Load KV full_docs with 1 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 42 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 98 data\n",
      "INFO:nano-graphrag:Loaded graph from ./nano_graphrag_cache_deepseek_TEST/graph_chunk_entity_relation.graphml with 569 nodes, 601 edges\n",
      "INFO:nano-vectordb:Load (556, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './nano_graphrag_cache_deepseek_TEST/vdb_entities.json'} 556 data\n",
      "INFO:nano-vectordb:Load (42, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './nano_graphrag_cache_deepseek_TEST/vdb_chunks.json'} 42 data\n"
     ]
    }
   ],
   "source": [
    "from nano_graphrag import GraphRAG, QueryParam\n",
    "from examples.using_deepseek_as_llm import (\n",
    "    deepseepk_model_if_cache,   # LLM ya listo y con cache opcional\n",
    "    local_st_embeddings,\n",
    "    WORKING_DIR,\n",
    ")\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# === Inicializa GraphRAG (ya indexaste antes con el mismo WORKING_DIR) ===\n",
    "rag = GraphRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    best_model_func=deepseepk_model_if_cache,\n",
    "    cheap_model_func=deepseepk_model_if_cache,\n",
    "    embedding_func=local_st_embeddings,\n",
    "    enable_naive_rag=True,\n",
    "    enable_llm_cache=False,\n",
    ")\n",
    "\n",
    "def show_md(text: str):\n",
    "    display(Markdown(f\"```\\n{text.strip()}\\n```\"))\n",
    "\n",
    "# === Naive (solo embeddings) ===\n",
    "async def ask_naive(q: str, max_tokens: int = 180):\n",
    "    prompt = f\"{q}\\n\\nResponde en máximo 5 líneas claras y concisas.\"\n",
    "    # OJO: no se pasan kwargs aquí; GraphRAG no acepta temperature/max_tokens.\n",
    "    return await rag.aquery(prompt, QueryParam(mode=\"naive\"))\n",
    "\n",
    "# === GraphRAG Global (comunidades) ===\n",
    "async def ask_global(q: str, max_tokens: int = 180):\n",
    "    prompt = f\"{q}\\n\\nResponde en máximo 5 líneas claras y concisas.\"\n",
    "    return await rag.aquery(prompt, QueryParam(mode=\"global\"))\n",
    "\n",
    "# === Puro LLM (sin retrieval) usando la función del example ===\n",
    "async def ask_llm_only(q: str, temperature: float = 0.7, max_tokens: int = 200):\n",
    "    prompt = f\"{q}\\n\\nResponde en máximo 5 líneas claras y concisas.\"\n",
    "    # Esta función ya llama a DeepSeek con AsyncOpenAI y maneja cache si lo activas\n",
    "    return await deepseepk_model_if_cache(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6d1310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "INFO:nano-graphrag:Truncate 20 to 9 chunks\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:Revtrieved 96 communities\n",
      "INFO:nano-graphrag:Grouping to 5 groups for global search\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Donde vive Scrooge?\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE RAG:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Según la información proporcionada, Scrooge vive en unas habitaciones que anteriormente pertenecieron a su socio fallecido, Jacob Marley. Se describe como un conjunto de habitaciones lúgubres en un edificio sombrío, situado en un patio. El texto especifica que es un lugar oscuro y que nadie más vivía en el edificio excepto Scrooge, ya que las demás habitaciones se alquilaban como oficinas.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRAPH RAG (GLOBAL):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Scrooge vive en Londres, específicamente en una zona conocida como The City. Su residencia es una casa lúgubre y oscura que anteriormente perteneció a su difunto socio, Marley. Esta vivienda se encuentra en un edificio envuelto en niebla y escarcha, lo que refleja su carácter sombrío.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Ejemplo de comparación ===\n",
    "question = \"Donde vive Scrooge?\"\n",
    "\n",
    "ans_naive  = await ask_naive(question)\n",
    "ans_global = await ask_global(question)\n",
    "# ans_llm    = await ask_llm_only(question, temperature=0.5, max_tokens=180)\n",
    "\n",
    "print(\"Pregunta:\");   show_md(question)\n",
    "print(\"NAIVE RAG:\\n\");   show_md(ans_naive)\n",
    "print(\"\\nGRAPH RAG (GLOBAL):\\n\"); show_md(ans_global)\n",
    "# print(\"\\nPURO LLM:\\n\");    show_md(ans_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e00f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
