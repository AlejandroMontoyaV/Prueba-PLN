{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "772b7331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Loading tokenizer: type='tiktoken', name='gpt-4o'\n",
      "INFO:nano-graphrag:Load KV full_docs with 1 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 42 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 98 data\n",
      "/opt/miniconda3/envs/graphrag/lib/python3.11/xml/etree/ElementTree.py:581: RuntimeWarning: coroutine 'GraphRAG.ainsert' was never awaited\n",
      "  self._root = parser._parse_whole(source)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "INFO:nano-graphrag:Loaded graph from ./nano_graphrag_cache_deepseek_TEST/graph_chunk_entity_relation.graphml with 569 nodes, 601 edges\n",
      "INFO:nano-vectordb:Load (556, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './nano_graphrag_cache_deepseek_TEST/vdb_entities.json'} 556 data\n",
      "INFO:nano-vectordb:Load (42, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './nano_graphrag_cache_deepseek_TEST/vdb_chunks.json'} 42 data\n"
     ]
    }
   ],
   "source": [
    "from nano_graphrag import GraphRAG, QueryParam\n",
    "from examples.using_deepseek_as_llm import (\n",
    "    deepseepk_model_if_cache,   # LLM ya listo y con cache opcional\n",
    "    local_st_embeddings,\n",
    "    WORKING_DIR,\n",
    ")\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# === Inicializa GraphRAG (ya indexaste antes con el mismo WORKING_DIR) ===\n",
    "rag = GraphRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    best_model_func=deepseepk_model_if_cache,\n",
    "    cheap_model_func=deepseepk_model_if_cache,\n",
    "    embedding_func=local_st_embeddings,\n",
    "    enable_naive_rag=True,\n",
    "    enable_llm_cache=False,\n",
    ")\n",
    "\n",
    "def show_md(text: str):\n",
    "    display(Markdown(f\"```\\n{text.strip()}\\n```\"))\n",
    "\n",
    "# === Naive (solo embeddings) ===\n",
    "async def ask_naive(q: str, max_tokens: int = 180):\n",
    "    prompt = f\"{q}\\n\\nResponde en m√°ximo 5 l√≠neas claras y concisas.\"\n",
    "    # OJO: no se pasan kwargs aqu√≠; GraphRAG no acepta temperature/max_tokens.\n",
    "    return await rag.aquery(prompt, QueryParam(mode=\"naive\"))\n",
    "\n",
    "# === GraphRAG Global (comunidades) ===\n",
    "async def ask_global(q: str, max_tokens: int = 180):\n",
    "    prompt = f\"{q}\\n\\nResponde en m√°ximo 5 l√≠neas claras y concisas.\"\n",
    "    return await rag.aquery(prompt, QueryParam(mode=\"global\"))\n",
    "\n",
    "# === Puro LLM (sin retrieval) usando la funci√≥n del example ===\n",
    "async def ask_llm_only(q: str, temperature: float = 0.7, max_tokens: int = 200):\n",
    "    prompt = f\"{q}\\n\\nResponde en m√°ximo 5 l√≠neas claras y concisas.\"\n",
    "    # Esta funci√≥n ya llama a DeepSeek con AsyncOpenAI y maneja cache si lo activas\n",
    "    return await deepseepk_model_if_cache(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d1310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 24.99it/s]\n",
      "INFO:nano-graphrag:Revtrieved 96 communities\n",
      "INFO:nano-graphrag:Grouping to 5 groups for global search\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© NAIVE RAG:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Sorry, I'm not able to provide an answer to that question.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê GRAPH RAG (GLOBAL):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Basado en la informaci√≥n disponible, el profesor Guzm√°n era un acad√©mico que manten√≠a un estudio en el mismo edificio que la oficina de Scrooge en la Universidad Nacional de Colombia, campus Medell√≠n. Esto indica que desarrollaba sus actividades en un entorno universitario.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† PURO LLM:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Guzm√°n es un nombre que puede referirse a varias figuras hist√≥ricas y contempor√°neas. La m√°s conocida internacionalmente es **Joaqu√≠n \"El Chapo\" Guzm√°n**, el narcotraficante mexicano fundador del C√°rtel de Sinaloa, quien fue capturado y extraditado a Estados Unidos. Tambi√©n puede hacer referencia a **Luis Guzm√°n**, un actor puertorrique√±o-estadounidense conocido por sus papeles en cine y televisi√≥n. En un contexto hist√≥rico, **Alonso P√©rez de Guzm√°n** fue un noble y militar espa√±ol conocido como \"Guzm√°n el Bueno\". Sin m√°s contexto, es imposible determinar a cu√°l se refiere espec√≠ficamente.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Ejemplo de comparaci√≥n ===\n",
    "question = \"Quien es Guzm√°n?\"\n",
    "\n",
    "ans_naive  = await ask_naive(question)\n",
    "ans_global = await ask_global(question)\n",
    "ans_llm    = await ask_llm_only(question, temperature=0.5, max_tokens=180)\n",
    "\n",
    "print(\"NAIVE RAG:\\n\");   show_md(ans_naive)\n",
    "print(\"NAIVE RAG:\\n\");   show_md(ans_naive)\n",
    "print(\"\\nGRAPH RAG (GLOBAL):\\n\"); show_md(ans_global)\n",
    "print(\"\\nPURO LLM:\\n\");    show_md(ans_llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
